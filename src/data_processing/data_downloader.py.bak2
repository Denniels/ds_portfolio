"""
Módulo para la obtención y actualización de datos desde SNICHILE.

Este módulo se encarga de:
1. Descargar datos desde la web de SNICHILE
2. Procesar los archivos descargados
3. Actualizar la base de datos local
"""

from datetime import datetime
import logging
import os
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError

# Configurar logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class SNICHILEDownloader:
    """
    Clase para descargar y procesar datos de SNICHILE.

    Attributes:
        base_url (str): URL base de SNICHILE
        data_dir (Path): Directorio para almacenar datos descargados
        db_engine: Conexión a la base de datos
    """

    def __init__(self, data_dir: Path):
        """
        Inicializa el descargador de datos.

        Args:
            data_dir (Path): Directorio para almacenar datos
        """
        self.base_url = "https://snichile.mma.gob.cl"
        self.data_dir = data_dir
        self.raw_dir = data_dir / "raw"
        self.raw_dir.mkdir(parents=True, exist_ok=True)

        # Cargar variables de entorno
        load_dotenv()

        # Crear conexión a la base de datos
        self.db_engine = self._create_db_connection()
        self._create_database_schema()

    def _create_db_connection(self):
        """
        Crea la conexión a la base de datos PostgreSQL.

        Returns:
            sqlalchemy.engine.Engine: Motor de conexión a la base de datos
        """
        try:
            db_user = os.getenv("POSTGRES_USER", "postgres")
            db_pass = os.getenv("POSTGRES_PASSWORD", "postgres")
            db_host = os.getenv("POSTGRES_HOST", "localhost")
            db_port = os.getenv("POSTGRES_PORT", "5432")
            db_name = os.getenv("POSTGRES_DB", "ds_portfolio")

            db_url = f"postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}"
            engine = create_engine(db_url)

            # Verificar conexión
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
                logger.info("Conexión a la base de datos establecida correctamente")

            return engine

        except SQLAlchemyError as e:
            msg = f"Error al conectar con la base de datos: {str(e)}"
            logger.error(msg)
            raise

    def _create_database_schema(self):
        """Crea el esquema de la base de datos si no existe."""
        try:
            # Crear tablas necesarias            
            queries = [
                """
                CREATE TABLE IF NOT EXISTS sectores_economicos (
                    sector_id SERIAL PRIMARY KEY,
                    nombre VARCHAR(100) NOT NULL,
                    descripcion TEXT
                );
                """,
                """
                CREATE TABLE IF NOT EXISTS regiones (
                    region_id SERIAL PRIMARY KEY,
                    nombre VARCHAR(100) NOT NULL,
                    codigo VARCHAR(10)
                );
                """,
                """
                CREATE TABLE IF NOT EXISTS emisiones_co2 (
                    id SERIAL PRIMARY KEY,
                    sector_id INTEGER REFERENCES sectores_economicos(sector_id),
                    region_id INTEGER REFERENCES regiones(region_id),
                    anio INTEGER NOT NULL,
                    valor_emision DECIMAL(12,2) NOT NULL,
                    unidad_medida VARCHAR(50),
                    fecha_carga TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    archivo_fuente VARCHAR(255)
                );
                """,
                """
                CREATE TABLE IF NOT EXISTS metadata_actualizacion (
                    id SERIAL PRIMARY KEY,
                    ultima_actualizacion TIMESTAMP,
                    fuente VARCHAR(200),
                    estado VARCHAR(50)
                );
                """,
            ]

            for query in queries:
                with self.db_engine.connect() as conn:
                    conn.execute(text(query))
                    conn.commit()

            logger.info("Esquema de base de datos creado correctamente")

        except SQLAlchemyError as e:
            logger.error(f"Error al crear esquema de base de datos: {str(e)}")
            raise

    def obtener_datasets_disponibles(self) -> List[Dict[str, str]]:
        """
        Obtiene la lista de datasets disponibles en SNICHILE.

        Returns:
            List[Dict[str, str]]: Lista de datasets con sus metadatos
        """
        try:
            url = f"{self.base_url}/datos-de-gei-chile"
            response = requests.get(url)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            datasets = []

            # Buscar tablas de datos
            data_tables = soup.find_all("table", class_="views-table")

            for table in data_tables:
                rows = table.find_all("tr")[1:]  # Ignorar encabezado
                for row in rows:
                    cols = row.find_all("td")
                    if len(cols) >= 2:
                        # Buscar enlaces de descarga
                        download_link = cols[-1].find("a")
                        if download_link and "href" in download_link.attrs:
                            datasets.append(
                                {
                                    "titulo": cols[0].get_text(strip=True),
                                    "anio": (
                                        cols[1].get_text(strip=True)
                                        if len(cols) > 2
                                        else ""
                                    ),
                                    "url": download_link["href"],
                                }
                            )

            logger.info(f"Se encontraron {len(datasets)} datasets disponibles")
            return datasets

        except Exception as e:
            logger.error(f"Error al obtener lista de datasets: {str(e)}")
            raise

    def descargar_dataset(self, dataset: Dict[str, str]) -> Optional[Path]:
        """
        Descarga un dataset específico.

        Args:
            dataset (Dict[str, str]): Información del dataset a descargar

        Returns:
            Optional[Path]: Ruta al archivo descargado o None si hubo error
        """
        try:
            url = dataset["url"]
            if not url.startswith("http"):
                url = f"{self.base_url}{url}"

            filename = (
                f"{dataset['anio']}_{dataset['titulo'].lower().replace(' ', '_')}.xlsx"
            )
            file_path = self.raw_dir / filename

            response = requests.get(url, stream=True)
            response.raise_for_status()

            with open(file_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            logger.info(f"Dataset descargado: {filename}")
            return file_path

        except Exception as e:
            logger.error(f"Error al descargar dataset {dataset['titulo']}: {str(e)}")
            return None

    def procesar_dataset(self, file_path: Path) -> pd.DataFrame:
        """
        Procesa un archivo de datos descargado.

        Args:
            file_path (Path): Ruta al archivo Excel

        Returns:
            pd.DataFrame: DataFrame con los datos procesados
        """
        try:
            # Detectar tipo de archivo
            if file_path.suffix.lower() == ".xlsx":
                df = pd.read_excel(file_path)
            elif file_path.suffix.lower() == ".csv":
                df = pd.read_csv(file_path)
            else:
                raise ValueError(f"Formato de archivo no soportado: {file_path.suffix}")

            # Limpiar nombres de columnas
            df.columns = df.columns.str.lower().str.replace(" ", "_")

            # Agregar metadatos
            df["fecha_carga"] = datetime.now()
            df["archivo_fuente"] = file_path.name

            logger.info(f"Archivo procesado: {file_path.name}")
            return df

        except Exception as e:
            logger.error(f"Error al procesar dataset {file_path}: {str(e)}")
            raise

    def guardar_en_db(self, df: pd.DataFrame, nombre_tabla: str) -> None:
        """
        Guarda un DataFrame en la base de datos.

        Args:
            df (pd.DataFrame): DataFrame a guardar
            nombre_tabla (str): Nombre de la tabla destino
        """
        try:
            df.to_sql(
                name=nombre_tabla,
                con=self.db_engine,
                if_exists="replace",
                index=False,
                chunksize=1000,
            )

            # Actualizar metadata
            metadata_df = pd.DataFrame(
                [
                    {
                        "ultima_actualizacion": datetime.now(),
                        "fuente": "SNICHILE",
                        "estado": "OK",
                    }
                ]
            )

            metadata_df.to_sql(
                "metadata_actualizacion",
                self.db_engine,
                if_exists="append",
                index=False,
            )

            logger.info(f"Datos guardados en tabla: {nombre_tabla}")

        except SQLAlchemyError as e:
            logger.error(f"Error al guardar en base de datos: {str(e)}")
            raise

    def sincronizar_datos(self) -> None:
        """
        Ejecuta el proceso completo de sincronización de datos:
        1. Obtiene lista de datasets disponibles
        2. Descarga cada dataset
        3. Procesa los datos
        4. Guarda en la base de datos
        """
        try:
            # Obtener datasets disponibles
            datasets = self.obtener_datasets_disponibles()
            logger.info(f"Iniciando sincronización de {len(datasets)} datasets")

            for dataset in datasets:
                # Descargar dataset
                file_path = self.descargar_dataset(dataset)
                if file_path:
                    # Procesar datos
                    df = self.procesar_dataset(file_path)

                    # Guardar en base de datos
                    nombre_tabla = f"emisiones_{dataset['anio']}"
                    self.guardar_en_db(df, nombre_tabla)

            logger.info("Sincronización completada exitosamente")

        except Exception as e:
            logger.error(f"Error durante la sincronización: {str(e)}")
            raise
